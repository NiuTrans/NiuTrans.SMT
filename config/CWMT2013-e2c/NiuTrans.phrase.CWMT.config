###########################################
### NiuTrans decoder configuration file ###
###          phrase-based system        ###
###              2011-07-01             ###
###########################################

#>>> runtime resource tables

# language model
param="Ngram-LanguageModel-File"     value="../work/lm/lm.trie.data"

# target-side vocabulary
param="Target-Vocab-File"            value="../work/lm/lm.vocab"

# MaxEnt-based lexicalized reordering model
param="ME-Reordering-Table"          value="../work/model/me.reordering.table"

# MSD lexicalized reordering model
param="MSD-Reordering-Model"         value="../work/model/msd.reordering.table.filterDevAndTest"

# phrase translation model
param="Phrase-Table"                 value="../work/model/phrase.translation.table.filterDevAndTest"

# Punctuations specified (sample file is in utf8.).
# Of course, users can modify/change this file as needed.
param="Punct-Vocab-File"             value="../resource/punct.utf8.txt"

#>>> runtime parameters

# number of MERT iterations
param="nround"                       value="12"

# order of n-gram language model
param="ngram"                        value="5"

# maximum phrase length
param="maxphraselength"              value="7"

# use punctuation pruning (1) or not (0)
param="usepuncpruning"               value="1"

# use cube-pruning (1) or not (0)
param="usecubepruning"               value="1"

# use maxent reordering model (1) or not (0)
param="use-me-reorder"               value="1"

# use msd reordering model (1) or not (0)
param="use-msd-reorder"              value="1"

# number of threads
param="nthread"                      value="8"

# how many translations are dumped
param="nbest"                        value="50"

# output OOV words
param="outputoov"                    value="0"

# output source words that are explicitly deleted
param="outputnull"                   value="0"

# beam size (or beam width)
param="beamsize"                     value="50"

# beam scale. This parameter controls the number of hypotheses
# that are computed in decoding.
# e.g., beamscale=3 means we search over 3 * beamsize hyphotheses
param="beamscale"                    value="1"

# number of references of dev. set
param="nref"                         value="4"

# allow null-translation (i.e. word-deletion)
param="usenulltrans"                 value="0"

# allow sequence of null-translations
param="snulltrans"                   value="1"

# normalize output (1) or not (0)
param="normalizeoutput"              value="0"

# distortion limit
param="maxdd"                        value="8"

# lowercase translation result
param="lowertext"                    value="1"

#>>> model parameters

# features used
#  0: n-gram language model
#  1: number of target-words
#  2: Pr(e|f). f->e translation probablilty.
#  3: Lex(e|f). f->e lexical weight
#  4: Pr(f|e). e->f translation probablilty.
#  5: Lex(f|e). e->f lexical weight
#  6: number of phrases
#  7: number of bi-lex links (not fired in current version)
#  8: number of NULL-translation (i.e. word deletion)
#  9: MaxEnt-based lexicalized reordering model
# 10: <UNDEFINED>
# 11: MSD reordering model: Previous & Monotonic
# 12: MSD reordering model: Previous & Swap
# 13: MSD reordering model: Previous & Discontinuous
# 14: MSD reordering model: Following & Monotonic
# 15: MSD reordering model: Following & Swap
# 16: MSD reordering model: Following & Discontinuous

# feature weights
param="weights"                      value="1.000 0.500 0.200 0.200 0.200 0.200 0.500 0.500 -0.100 1.000 0.000 0.100 0.100 0.100 0.100 0.100 0.100"

# bound the feature weight in MERT
# e.g. the first number "-3:7" means that the first feature weight ranges in [-3, 7]
param="ranges"                       value="-3:7 -1:3 0:3 0:0.4 0:3 0:0.4 -3:3 -3:3 -3:0 -3:3 0:0 0:3 0:0.3 0:0.3 0:3 0:0.3 0:0.3"

# fix a dimention (1) or not (0)
param="fixedfs"                      value="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0"

# optimized weights tuned by MERT training
param="weights"                      value=" 2.0080 1.6689 0.9979 0.1577 0.5808 0.3900 0.9585 0.9100 -0.0100 2.6972 0.0000 0.4922 0.0258 0.3000 0.0803 0.0848 0.3000"